{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Package version:\n",
        "- transformers: 4.47.0.dev0\n",
        "- torch: 2.4.0+cu121\n",
        "- torch_xla: 2.4.0+libtpu"
      ],
      "metadata": {
        "id": "YR05AEoHsYIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Login to HuggingFace for downloading a gated model and uploading trained model"
      ],
      "metadata": {
        "id": "naUbg0Y9seQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token your_hf_token_here"
      ],
      "metadata": {
        "id": "3PUQyYbzsete"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install package"
      ],
      "metadata": {
        "id": "pcFpvKgZsuDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install \"huggingface_hub[hf_transfer]\"\n",
        "!pip3 install datasets peft -q\n",
        "!pip install git+https://github.com/huggingface/transformers.git -qq\n",
        "!pip install --force-reinstall -v torch==2.4.0 --index-url https://download.pytorch.org/whl/cpu -q\n",
        "!pip install --force-reinstall -v torch_xla[tpu]==2.4.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n",
        "!pip uninstall tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT"
      ],
      "metadata": {
        "id": "RbjgbO5_sub_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supported model for TPU SPMD:\n",
        "- GPTNeoX\n",
        "- T5\n",
        "- Llama\n",
        "- CLIP\n",
        "- CLIPVision\n",
        "- Llava\n",
        "- Gemma\n",
        "- Mistral\n",
        "- GPT2\n",
        "- Qwen2\n",
        "- Mixtral\n",
        "- Phi"
      ],
      "metadata": {
        "id": "yixrO8pYsxIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import torch_xla.distributed.spmd as xs\n",
        "from transformers import logging as hf_logging\n",
        "import torch_xla.runtime as xr\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "xr.use_spmd()\n",
        "from torch_xla.distributed.spmd import Mesh\n",
        "import torch.nn as nn\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import (\n",
        "    GPTNeoXConfig, T5Config, LlamaConfig, CLIPConfig, CLIPVisionConfig, LlavaConfig, GemmaConfig,\n",
        "    MistralConfig, GPT2Config, Qwen2Config, MixtralConfig, PhiConfig, AutoTokenizer, AutoModelForCausalLM,\n",
        "    AutoModelForSequenceClassification, AutoConfig, Gemma2Config\n",
        ")\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from datasets import Dataset, load_dataset, concatenate_datasets\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "import transformers\n",
        "import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "import random\n",
        "\n",
        "# ends with $ to prevent sharding lora parameters\n",
        "GPTNEOX_RULES = (\n",
        "    # embeddings\n",
        "    (\"gpt_neox\\\\.embed_in\", (\"mp\", \"fsdp\")),\n",
        "\n",
        "    # atention\n",
        "    (\"attention\\\\.query_key_value$\", (\"fsdp\", \"mp\")),\n",
        "    (\"attention\\\\.dense$\", (\"mp\", \"fsdp\")),\n",
        "\n",
        "    # mlp\n",
        "    (\"mlp\\\\.dense_h_to_4h$\", (\"fsdp\", \"mp\")),\n",
        "    (\"mlp\\\\.dense_4h_to_h$\", (\"mp\", \"fsdp\")),\n",
        "\n",
        "    # output\n",
        "    (\"embed_out\", (\"fsdp\", \"mp\")),\n",
        ")\n",
        "T5_RULES = (\n",
        "    # embeddings\n",
        "    (\"shared$\", (\"mp\", \"fsdp\")),\n",
        "    (\"embed_tokens$\", (\"mp\", \"fsdp\")),\n",
        "\n",
        "    # attention\n",
        "    (\"q$\", (\"fsdp\", \"mp\")),\n",
        "    (\"k$\", (\"fsdp\", \"mp\")),\n",
        "    (\"v$\", (\"fsdp\", \"mp\")),\n",
        "    (\"o$\", (\"mp\", \"fsdp\")),\n",
        "\n",
        "    # mlp\n",
        "    (\"w$\", (\"fsdp\", \"mp\")),\n",
        "    (\"wi_0$\", (\"fsdp\", \"mp\")),\n",
        "    (\"wi_1$\", (\"fsdp\", \"mp\")),\n",
        "    (\"wo$\", (\"mp\", \"fsdp\")),\n",
        "\n",
        "    # seq2seq lm head\n",
        "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
        ")\n",
        "LLAMA_RULES = (\n",
        "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
        "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
        "    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n",
        "    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n",
        "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
        ")\n",
        "CLIP_RULES = (\n",
        "    (\"patch_embedding$\", (\"fsdp\", \"mp\", None, None)),\n",
        "    (\"position_embedding$\", (\"mp\", \"fsdp\")),\n",
        "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)$\", (\"fsdp\", \"mp\")),\n",
        "    (\"self_attn\\\\.out_proj$\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.fc1$\", (\"fsdp\", \"mp\")),\n",
        "    (\"mlp\\\\.fc2$\", (\"mp\", \"fsdp\")),\n",
        "    (\"visual_projection$\", (\"fsdp\", \"mp\")),\n",
        "    (\"text_projection$\", (\"fsdp\", \"mp\")),\n",
        ")\n",
        "LLAVA_RULES = (\n",
        "    (\"multi_modal_projector\\\\.linear_1$\", (\"fsdp\", \"mp\")),\n",
        "    (\"multi_modal_projector\\\\.linear_2$\", (\"mp\", \"fsdp\")),\n",
        "    *LLAMA_RULES,\n",
        "    *CLIP_RULES,\n",
        ")\n",
        "GEMMA_RULES = (\n",
        "    (\"model\\\\.embed_tokens\", (\"mp\", (\"fsdp\", \"sp\"))),\n",
        "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", ((\"fsdp\", \"sp\"), \"mp\")),\n",
        "    (\"self_attn\\\\.o_proj\", (\"mp\", (\"fsdp\", \"sp\"))),\n",
        "    (\"mlp\\\\.gate_proj\", ((\"fsdp\", \"sp\"), \"mp\")),\n",
        "    (\"mlp\\\\.down_proj\", (\"mp\", (\"fsdp\", \"sp\"))),\n",
        "    (\"mlp\\\\.up_proj\", ((\"fsdp\", \"sp\"), \"mp\")),\n",
        "    (\"lm_head\", ((\"fsdp\", \"sp\"), \"mp\")),\n",
        "    (\"score\", ((\"fsdp\", \"sp\"), \"mp\")),\n",
        ")\n",
        "GPT2_RULES = (\n",
        "    # embeddings\n",
        "    (\"wte\", (\"mp\", \"fsdp\")),\n",
        "    (\"wpe\", (\"mp\", \"fsdp\")),\n",
        "\n",
        "    # attention\n",
        "    (\"c_attn\", (\"fsdp\", \"mp\")),\n",
        "    (\"c_proj\", (\"mp\", \"fsdp\")),\n",
        "\n",
        "    # mlp\n",
        "    (\"c_fc\", (\"fsdp\", \"mp\")),\n",
        "    (\"c_proj\", (\"mp\", \"fsdp\")),\n",
        "\n",
        "    # output\n",
        "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
        ")\n",
        "QWEN_RULES = (\n",
        "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
        "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
        "    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n",
        "    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n",
        "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
        ")\n",
        "MIXTRAL_RULES = (\n",
        "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
        "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
        "    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n",
        "    (\"w1\", (\"fsdp\", \"mp\")),\n",
        "    (\"w2\", (\"mp\", \"fsdp\")),\n",
        "    (\"w3\", (\"fsdp\", \"mp\")),\n",
        "    (\"gate\", (\"mp\", \"fsdp\")),\n",
        "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
        ")\n",
        "PHI_RULES = (\n",
        "    ### (regex) linear modules, (list[sharding methods]) )\n",
        "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
        "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
        "    (\"self_attn\\\\.dense\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.fc2\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.fc1\", (\"fsdp\", \"mp\")),\n",
        "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
        ")\n",
        "ALL_RULES = [\n",
        "    (GPTNeoXConfig, GPTNEOX_RULES),\n",
        "    (T5Config, T5_RULES),\n",
        "    (LlamaConfig, LLAMA_RULES),\n",
        "    (CLIPConfig, CLIP_RULES),\n",
        "    (CLIPVisionConfig, CLIP_RULES),\n",
        "    (LlavaConfig, LLAVA_RULES,),\n",
        "    (GemmaConfig, GEMMA_RULES),\n",
        "    (MistralConfig, LLAMA_RULES),\n",
        "    (GPT2Config, GPT2_RULES),\n",
        "    (Qwen2Config, QWEN_RULES),\n",
        "    (MixtralConfig, MIXTRAL_RULES),\n",
        "    (PhiConfig,PHI_RULES),\n",
        "    (Gemma2Config, GEMMA_RULES)\n",
        "]\n",
        "\n",
        "def find_rule(model):\n",
        "    for config, rule in ALL_RULES:\n",
        "        if model.config.__class__ == config:\n",
        "            return rule\n",
        "    raise Exception(\"unsupported model to partitioning \" + str(model.config.__class__))\n",
        "\n",
        "def partition_module(model, mesh, device='xla', verbose=True):\n",
        "    partition_specs = find_rule(model)\n",
        "    model.to(device)\n",
        "\n",
        "    for name, module in (tqdm(model.named_modules(), desc=\"partitioning model\", disable=not verbose, position=0)):\n",
        "        if not hasattr(module, \"weight\") or not isinstance(module.weight, nn.Parameter):\n",
        "            continue\n",
        "        find = False\n",
        "        # print(name, module.__class__.__name__)\n",
        "        for rule_pattern, spec in partition_specs:\n",
        "            if re.findall(rule_pattern, name):\n",
        "                if verbose:\n",
        "                    print(\"match\", rule_pattern, name, spec)\n",
        "                    print(f\"y match {module}\", name, module.weight.size(), module.weight.dim())\n",
        "                xs.mark_sharding(module.weight, mesh, spec)\n",
        "                find = True\n",
        "                break\n",
        "\n",
        "        if not find:\n",
        "            if verbose:\n",
        "                print(f\"no match {module}\", name, module.weight.size(), module.weight.dim())\n",
        "            xs.mark_sharding(module.weight, mesh, tuple([None] * module.weight.dim()))\n",
        "\n",
        "!export USE_TORCH=True\n",
        "!export XLA_USE_BF16=1\n",
        "os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n",
        "try:\n",
        "    os.environ.pop('TPU_PROCESS_ADDRESSES')\n",
        "except:\n",
        "    pass\n",
        "hf_logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "lLOvBgyws3Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def set_seeds(seed: int=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    transformers.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seeds(42)"
      ],
      "metadata": {
        "id": "f4vwPqZ1tCqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here i use Llama 3.2 3B model and trained it on unstructured medical corpus text data"
      ],
      "metadata": {
        "id": "cLChay1BtLLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download meta-llama/Llama-3.2-3B\n",
        "model_name = 'meta-llama/Llama-3.2-3B'\n",
        "BATCH_SIZE = 4\n",
        "epochs = 2\n",
        "seq_length = 4096"
      ],
      "metadata": {
        "id": "mIk6M9cBtJn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "medicaltextbook = []\n",
        "medicaltextbookconfig = datasets.get_dataset_config_names(\"zxvix/MedicalTextbook\")\n",
        "for i in medicaltextbookconfig:\n",
        "    if not any(word in i for word in [\"128\", \"annotate\", \"paraphrase\", \"paraphrase_2\", \"paraphrase_3\", \"summary\", \"summary_long\"]):\n",
        "        medicaltextbook.extend(load_dataset(\"zxvix/MedicalTextbook\", i)['train']['text'])\n",
        "\n",
        "medicaltranscription = load_dataset(\"rungalileo/medical_transcription_40\")[\"train\"][\"text\"]\n",
        "medicaltranscription.extend(load_dataset(\"rungalileo/medical_transcription_40\")[\"test\"][\"text\"])\n",
        "\n",
        "wikimedicalterms = load_dataset(\"gamino/wiki_medical_terms\")[\"train\"][\"page_text\"]\n",
        "\n",
        "fulldataset = medicaltextbook + medicaltranscription + wikimedicalterms\n",
        "fulldatasettoken = tokenizer(fulldataset)['input_ids']"
      ],
      "metadata": {
        "id": "jPZQqHUUtRP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [[0]]\n",
        "for i in tqdm(fulldatasettoken):\n",
        "    if len(dataset[-1]) + len(i) <= seq_length:\n",
        "        dataset[-1].extend(i[1:])\n",
        "    else:\n",
        "        if len(i) <= seq_length:\n",
        "            dataset.append(i[1:])\n",
        "        else:\n",
        "            j = 0\n",
        "            while len(i[j:]) > seq_length:\n",
        "                dataset.append(i[1+j:seq_length+j])\n",
        "                j += seq_length\n",
        "random.shuffle(dataset)"
      ],
      "metadata": {
        "id": "jDydYHQ2tg-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "for i in tqdm(fulldatasettoken):\n",
        "    total += len(i)\n",
        "print(f\"{total/1000000}M Tokens\")"
      ],
      "metadata": {
        "id": "Rroa4aRYtrPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating PyTorch dataset and dataloader"
      ],
      "metadata": {
        "id": "Tnd2ofsBtwOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "        def __init__(self,tokenizer,dataset):\n",
        "            super().__init__()\n",
        "            self.tokenizer = tokenizer\n",
        "            self.dataset = dataset\n",
        "            self.data = self.dataset\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "        def __getitem__(self, idx):\n",
        "            text = self.data[idx]\n",
        "            text = self.tokenizer.decode(text)\n",
        "            tokenize = self.tokenizer(text, max_length=4096, padding='max_length', truncation=True)\n",
        "            input_ids = torch.tensor(tokenize['input_ids'])\n",
        "            attn_masks = torch.tensor(tokenize['attention_mask'])\n",
        "            labels = torch.tensor(tokenize['input_ids']).clone().detach()\n",
        "            labels[labels == tokenizer.pad_token_id] = -100\n",
        "            return (input_ids, attn_masks, labels)\n",
        "datasetth = Dataset(tokenizer, dataset)\n",
        "dataloader = torch.utils.data.DataLoader(datasetth, batch_size=BATCH_SIZE)\n",
        "device = xm.xla_device()\n",
        "dataloader = pl.MpDeviceLoader(dataloader, device)"
      ],
      "metadata": {
        "id": "r0uKlwsqt1TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying SPMD"
      ],
      "metadata": {
        "id": "G39S65-Gt_27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_xla.distributed.fsdp.utils import apply_xla_patch_to_nn_linear\n",
        "\n",
        "model = apply_xla_patch_to_nn_linear(model, xs.xla_patched_nn_linear_forward)\n",
        "config = transformers.AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "num_devices = xr.global_runtime_device_count()\n",
        "mesh_shape = (1, num_devices, 1)\n",
        "device_ids = np.array(range(num_devices))\n",
        "mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n",
        "\n",
        "partition_module(model, mesh)"
      ],
      "metadata": {
        "id": "WAVIHaeVt_cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the training loop and automatically upload the model to HF repo"
      ],
      "metadata": {
        "id": "bKxBfeJGuN4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def save_model(model, tokenizer, optimizer, scheduler):\n",
        "    model = model.cpu()\n",
        "    model.push_to_hub(\"Llama-3.2-3B-contpretrain-Medical\")\n",
        "\n",
        "def train_loop(training_loader, optimizer, scheduler):\n",
        "    model.train()\n",
        "    for i in range(epochs):\n",
        "        step = 1\n",
        "        for batch in tqdm(training_loader):\n",
        "            input_ids, attention_mask, labels = batch[0], batch[1], batch[2]\n",
        "            xs.mark_sharding(input_ids, mesh, (0, 1))\n",
        "            xs.mark_sharding(attention_mask, mesh, (0, 1))\n",
        "            xs.mark_sharding(labels, mesh, (0, 1))\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            if step % 100 == 0:\n",
        "                xm.master_print(outputs.loss.detach().cpu().item())\n",
        "            del input_ids, attention_mask\n",
        "            outputs.loss.backward()\n",
        "            del outputs\n",
        "            optimizer.step()\n",
        "            xm.mark_step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            step += 1\n",
        "    save_model(model, tokenizer, optimizer, scheduler)\n",
        "\n",
        "base_lr = 6e-5\n",
        "warmup_steps = 100\n",
        "epsilon = 1e-8\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = base_lr, eps = epsilon)\n",
        "\n",
        "total_steps = len(dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=warmup_steps,\n",
        "                                            num_training_steps=total_steps)\n",
        "def train():\n",
        "    print(f\"\"\"Training {model}\"\"\")\n",
        "    train_loop(dataloader, optimizer, scheduler)"
      ],
      "metadata": {
        "id": "tmUM9M2QuEEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "XFczXYcRuMY5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}